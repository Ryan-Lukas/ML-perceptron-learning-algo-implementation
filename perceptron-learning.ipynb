{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv as csv\nimport random\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setup with algorithms**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def newTable(data):\n    max = 0\n    count = 0\n    \n    with open(data, 'r', newline ='') as f:\n        reader = csv.reader(f, delimiter=' ')\n        for line in reader:\n            count +=1\n            \n            for i in line:\n                index = int(i.split(\":\")[0])\n                \n                if index > max:\n                    max = index\n    return np.zeros((count, max+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillTable(table,df):\n    with open(df, 'r') as f:\n        reader = csv.reader(f, delimiter=\" \")\n        row = -1\n        for i in reader:\n            row +=1\n            \n            for o in i:\n                data = o.split(\":\")\n                \n                if(int(data[0]) == 0 or int(data[0])==1):\n                    table[row, 0] = int(data[0])\n                else:\n                    table[row, int(data[0])] = int(data[1])\n\n    return table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createNPY(input,output):\n    print(f\"{input}\")\n    print(f\"{output}\")\n    table = newTable(input)\n    fillTable(table,input)\n    print(\"Done\")\n    return table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(x,w,b):\n  return x.dot(w) + b\n\ndef create_weights(length):\n  w = np.zeros((length,1))\n  for i in range(length):\n    w[i] = random.uniform(-0.01,0.01)\n  b = random.uniform(-0.01,0.01)\n  return w,b\n\ndef update_weights(w, b, x, y, n):\n  w = w.T\n  \n  indent = 1\n  if y == 0:\n    indent = -1\n  w = w + (n * indent * x)\n  b = b + (n * indent)\n  return w.T, b\n\ndef splitdata(data):\n  \n  #data_x = data.iloc[:,1:] #columns\n  #ata_y = data.iloc[:,0] #labels\n  #for numpy\n  data_x = data[:,1:] #columns\n  data_y = data[:,0] #labels\n\n  return data_x, data_y\n\ndef accuracy(labels, predictions):\n  correct = 0\n\n  for i in range(len(labels)):\n    if labels[i] == predictions[i]:\n      correct += 1\n\n  return correct / float(len(labels)) * 100.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = features y= labels, n = learning rate, e = epochs\ndef simple_batch_perceptron_decay(x, y, n, e):\n\n  #features = x.to_numpy(copy=True)\n  #labels = y.to_numpy(copy=True)\n  features = x\n  labels = y\n  \n\n  length = features.shape[1]\n  #print(length)\n  w, b = create_weights(length) # TODO create_weights: this will create some value array of each w\n  \n  for epochs in range(e):\n\n    n = n /(1+epochs)\n    for i in range(features.shape[0]-1):\n      y_prime = predict(features[i,0:length],w,b) #TODO create predict\n    \n\n      #y_i = 1, w^tx <=0 sign = -1\n      #mistake on positive\n      if y_prime >= 0 and labels[i] == 0:\n        w, b = update_weights(w, b, features[i, 0:length],labels[i],n)\n    \n      #y_i = -1, w^tx >=0 sign = 1\n      #mistake on negative\n      elif y_prime <= 0 and labels[i] > 0:\n        w, b = update_weights(w, b, features[i,0:length],labels[i],n)\n  return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_batch_perceptron(x, y, n, e):\n\n  features = x\n  labels = y\n\n  length = features.shape[1]\n  #print(length)\n  w, b = create_weights(length) # TODO create_weights: this will create some value array of each w\n  \n  for epochs in range(e):\n    for i in range(features.shape[0]-1):\n      y_prime = predict(features[i,0:length],w,b) #TODO create predict\n    \n\n      #y_i = 1, w^tx <=0 sign = -1\n      #mistake on positive\n      if y_prime >= 0 and labels[i] < 0:\n        w, b = update_weights(w, b, features[i, 0:length],labels[i],n)\n    \n      #y_i = -1, w^tx >=0 sign = 1\n      #mistake on negative\n      elif y_prime <= 0 and labels[i] > 0:\n        w, b = update_weights(w, b, features[i,0:length],labels[i],n)\n\n  return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perceptron(x, w, b):\n  length = x.shape[1]\n  predictions = np.zeros((x.shape[0], 1))\n\n  for i in range(len(x)):\n    prediction = predict(x[i,0:length], w, b)\n    #prediction = predict(x.iloc[i,0:length], w, b)\n    #print(prediction)\n    if prediction > 0:\n      predictions[i] = 1\n    else:\n      predictions[i] = 0\n  \n  return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_perceptron(test_folds,train_folds,n,epochs):\n  \n  #split train folds\n  train_folds_x,train_folds_y = splitdata(train_folds)\n  \n  #start training\n  w,b = simple_batch_perceptron(train_folds_x,train_folds_y,n,epochs)\n  \n  #test\n  test_folds_x,test_folds_y = splitdata(test_folds)\n  s = perceptron(test_folds_x,w,b)\n  #percent = accuracy(test_folds_y,s)\n\n\n  #print(f\"n: {n}, epochs: {epochs}, accuracy: {percent}\")\n    \n  return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_perceptron_simple(test_folds,train_folds,n,epochs):\n  \n  #split train folds\n  train_folds_x,train_folds_y = splitdata(train_folds)\n  \n  #start training\n  w,b = simple_batch_perceptron_decay(train_folds_x,train_folds_y,n,epochs)\n  \n  #test\n  test_folds_x,test_folds_y = splitdata(test_folds)\n  s = perceptron(test_folds_x,w,b)\n  #percent = accuracy(test_folds_y,s)\n\n\n  #print(f\"n: {n}, epochs: {epochs}, accuracy: {percent}\")\n    \n  return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillTableWOint(table,df):\n    with open(df, 'r') as f:\n        reader = csv.reader(f, delimiter=\" \")\n        row = -1\n        for i in reader:\n            row +=1\n            \n            for o in i:\n                data = o.split(\":\")\n                \n                if(int(data[0]) == 0 or int(data[0])==1):\n                    table[row, 0] = int(data[0])\n                else:\n                    table[row, int(data[0])] = data[1]\n\n    return table\n\ndef createNPYWOint(input,output):\n    print(f\"{input}\")\n    print(f\"{output}\")\n    table = newTable(input)\n    fillTableWOint(table,input)\n    print(\"Done\")\n    return table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing bag of words dataset with simple batch perceptron**"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/bag-of-words/bow.train.libsvm'\ntestDataCSV = '/kaggle/working/bow.train.csv'\n\ntrain = createNPY(input_file,testDataCSV)\n\ninput_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/bag-of-words/bow.test.libsvm'\ntestDataCSV = '/kaggle/working/bow.test.csv'\n\ntest = createNPY(input_file,testDataCSV)\n\n#-----TO DO-----\n# check which perceptron algo/learning rate/epochs is best for each one against training and testing data, then train and test against eval\n\n#test_perceptron(test,train,1,20)\n#test_perceptron(test,train,.1,20)\n#test_perceptron(test,train,.01,20)\n\n\n\n\ninput_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/bag-of-words/bow.eval.anon.libsvm'\ntestDataCSV = '/kaggle/working/bow.eval.anon.csv'\n\nevalTable = createNPY(input_file,testDataCSV)\n\n\n\ns = test_perceptron(evalTable,train,1,10)\nprint(s.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id = '/kaggle/input/uofu-ml-fall-2020/project_data/data/eval.ids'\nexample = np.genfromtxt(id, delimiter = '\\n')\nexample = example.reshape(s.shape[0], 1)\n\noutput = np.concatenate((example, s), axis = 1).astype(int)\n  \npd.DataFrame(output).to_csv(\"/kaggle/working/bow.eval.prediction.csv\", header=[\"example_id\", \"label\"], index=None)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing tfidf dataset with simple batch perceptron**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a dumby test, going to use glove instead\ninput_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/tfidf/tfidf.train.libsvm'\ntestDataCSV = '/kaggle/working/tfidf.train.csv'\n\ntrain = createNPYWOint(input_file,testDataCSV)\n\ninput_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/tfidf/tfidf.eval.anon.libsvm'\ntestDataCSV = '/kaggle/working/tfidf.test.csv'\n\nevalTable = createNPYWOint(input_file,testDataCSV)\n\n\n\n#-----TO DO-----\n# check which perceptron algo/learning rate/epochs is best for each one against training and testing data, then train and test against eval\n\n#s = test_perceptron(test,train,1,20)\n#test_perceptron(test,train,.1,20)\n#test_perceptron(test,train,.01,20)\n#print(s.shape)\n\ns = test_perceptron(evalTable,train,1,10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id = '/kaggle/input/uofu-ml-fall-2020/project_data/data/eval.ids'\nexample = np.genfromtxt(id, delimiter = '\\n')\nexample = example.reshape(s.shape[0], 1)\n\nprint(example.shape[1])\nprint(s)\noutput = np.concatenate((example, s), axis = 1).astype(int)\n  \npd.DataFrame(output).to_csv(\"/kaggle/working/tfidf.eval.prediction.csv\", header=[\"example_id\", \"label\"], index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Testing tfidf with simple batch perceptron decay****"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninput_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/tfidf/tfidf.train.libsvm'\ntestDataCSV = '/kaggle/working/tfidf.train.csv'\n\ntrain = createNPYWOint(input_file,testDataCSV)\n\ninput_file = '/kaggle/input/uofu-ml-fall-2020/project_data/data/tfidf/tfidf.eval.anon.libsvm'\ntestDataCSV = '/kaggle/working/tfidf.test.csv'\n\nevalTable = createNPYWOint(input_file,testDataCSV)\n\n\ns = test_perceptron_simple(evalTable,train,.01,10)\n\nid = '/kaggle/input/uofu-ml-fall-2020/project_data/data/eval.ids'\nexample = np.genfromtxt(id, delimiter = '\\n')\nexample = example.reshape(s.shape[0], 1)\n\noutput = np.concatenate((example, s), axis = 1).astype(int)\n  \npd.DataFrame(output).to_csv(\"/kaggle/working/tfidf.eval.prediction_simple.csv\", header=[\"example_id\", \"label\"], index=None)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}